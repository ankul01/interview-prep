<!doctype html><html lang=en dir=auto data-theme=light><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Rate Limiter — System Design | Leadership Learning</title><meta name=keywords content><meta name=description content="High-level design for a rate limiter: algorithms, data model, storage, and failure modes."><meta name=author content="Ankul Choudhry"><link rel=canonical href=https://ankul01.github.io/leadership-learning/system-design/hld/rate-limiter/><link crossorigin=anonymous href=/leadership-learning/assets/css/stylesheet.7049f02f6bd80879931714fa362aaca2b5c749963e1f076603da1b4232a27b5c.css integrity="sha256-cEnwL2vYCHmTFxT6NiqsorXHSZY+HwdmA9obQjKie1w=" rel="preload stylesheet" as=style><link rel=icon href=https://ankul01.github.io/leadership-learning/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ankul01.github.io/leadership-learning/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ankul01.github.io/leadership-learning/favicon-32x32.png><link rel=apple-touch-icon href=https://ankul01.github.io/leadership-learning/apple-touch-icon.png><link rel=mask-icon href=https://ankul01.github.io/leadership-learning/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ankul01.github.io/leadership-learning/system-design/hld/rate-limiter/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"&&(document.querySelector("html").dataset.theme="dark")</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel=stylesheet><meta property="og:url" content="https://ankul01.github.io/leadership-learning/system-design/hld/rate-limiter/"><meta property="og:site_name" content="Leadership Learning"><meta property="og:title" content="Rate Limiter — System Design"><meta property="og:description" content="High-level design for a rate limiter: algorithms, data model, storage, and failure modes."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="system-design"><meta name=twitter:card content="summary"><meta name=twitter:title content="Rate Limiter — System Design"><meta name=twitter:description content="High-level design for a rate limiter: algorithms, data model, storage, and failure modes."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"System Design Preparation","item":"https://ankul01.github.io/leadership-learning/system-design/"},{"@type":"ListItem","position":2,"name":"HLD","item":"https://ankul01.github.io/leadership-learning/system-design/hld/"},{"@type":"ListItem","position":3,"name":"Rate Limiter — System Design","item":"https://ankul01.github.io/leadership-learning/system-design/hld/rate-limiter/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Rate Limiter — System Design","name":"Rate Limiter — System Design","description":"High-level design for a rate limiter: algorithms, data model, storage, and failure modes.","keywords":[],"articleBody":" Iteration: v1 — Basic Design Next: Distributed rate limiting, multi-tier strategies, adaptive throttling\n1. Problem Statement A rate limiter controls the number of requests a client can send to a server within a defined time window. Without one, a single client (malicious or buggy) can overwhelm the system, degrading service for everyone.\nWhen do you need a rate limiter? Scenario Example Prevent abuse / DDoS Bot hammering login endpoint Protect shared resources Database connection pool exhaustion Enforce billing tiers Free tier: 100 req/min, Pro: 10K req/min Ensure fairness One tenant on a multi-tenant platform shouldn’t starve others Cost control Upstream API charges per call (e.g., OpenAI, Twilio) 2. Requirements 2.1 Functional Requirements FR1: Limit requests per client (identified by user ID, API key, or IP) to N requests per time window. FR2: Return a clear rejection response (HTTP 429) when the limit is exceeded. FR3: Support configurable limits — different endpoints or user tiers can have different limits. FR4: Include rate limit metadata in response headers (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset). 2.2 Non-Functional Requirements (Basic) NFR Target Why it matters Latency \u003c 1ms per check Rate limiter sits in the hot path of every request Accuracy Best-effort (slight over-count is acceptable) Exact counting is expensive; a few extra requests slipping through is fine Availability Should not become a single point of failure If the limiter goes down, we need a fallback policy (fail-open vs fail-closed) Memory Bounded, proportional to active clients Can’t allocate unbounded memory per IP/user Parking for v2: Strong consistency across nodes, geo-distributed limiting, sub-second window granularity.\n3. High-Level Architecture (v1 — Single Node) ┌─────────────────────────┐ │ Client │ └────────────┬────────────┘ │ ▼ ┌─────────────────────────┐ │ API Gateway / │ │ Reverse Proxy │ │ (e.g., Nginx, Envoy) │ └────────────┬────────────┘ │ ▼ ┌─────────────────────────┐ │ Rate Limiter │ │ Middleware │ │ │ │ ┌───────────────────┐ │ │ │ In-Memory Store │ │ │ │ (HashMap/Cache) │ │ │ └───────────────────┘ │ └────────────┬────────────┘ │ ┌──────┴──────┐ │ │ ALLOWED REJECTED │ │ ▼ ▼ ┌───────────┐ ┌──────────┐ │ App │ │ HTTP 429 │ │ Server │ │ Too Many │ │ │ │ Requests │ └───────────┘ └──────────┘ Where does the rate limiter live? Option Pros Cons Client-side Reduces unnecessary network calls Can be bypassed; not trustworthy Server middleware Simple to implement; colocated with app logic Coupled to app; doesn’t protect upstream API Gateway Centralized; language-agnostic; protects all services Another infra component to manage v1 decision: Server middleware (simplest to start). We’ll move to API Gateway in v2.\n4. Rate Limiting Algorithms This is the core of the design. There are 4 well-known algorithms. Let’s understand each and pick one for v1.\n4.1 Fixed Window Counter How it works: Divide time into fixed windows (e.g., each minute). Maintain a counter per client per window. Increment on each request. Reject if counter \u003e limit.\nWindow: [12:00:00 — 12:01:00] Counter: 0 → 1 → 2 → ... → 100 → REJECT Window: [12:01:00 — 12:02:00] Counter resets to 0 Data structure:\nKey: \"{client_id}:{window_start_timestamp}\" Value: count (integer) Pros Cons Dead simple Boundary burst problem: 100 requests at 12:00:59 + 100 at 12:01:00 = 200 in 2 seconds Low memory (1 entry per client per window) Unfair at window edges 4.2 Sliding Window Log How it works: Store the timestamp of every request. On a new request, remove timestamps older than now - window_size. If remaining count \u003e= limit, reject.\nTimestamps: [12:00:01, 12:00:05, 12:00:12, ..., 12:00:58] New request at 12:01:02 → remove everything before 12:00:02 → count remaining Data structure:\nKey: \"{client_id}\" Value: sorted set of timestamps Pros Cons Perfectly accurate sliding window High memory — stores every timestamp No boundary burst problem Cleanup overhead on every request 4.3 Sliding Window Counter (Hybrid) How it works: Combine fixed window counter with a weighted overlap from the previous window.\nPrevious window count: 80 (window was 60s, 45s have passed into current) Current window count: 20 (15s into current window) Estimated count = 80 × (45/60) + 20 = 60 + 20 = 80 Pros Cons Low memory (two counters per client) Approximate (but very close in practice) Smooths out boundary bursts Slightly more complex logic 4.4 Token Bucket How it works: Each client has a bucket with capacity B tokens. Tokens are added at rate R per second. Each request consumes 1 token. If the bucket is empty, reject.\nBucket capacity: 10 Refill rate: 2 tokens/sec [t=0] Bucket: 10 → Request arrives → Bucket: 9 ✓ [t=0] Bucket: 9 → Request arrives → Bucket: 8 ✓ ... [t=0] Bucket: 0 → Request arrives → REJECTED ✗ [t=1] Bucket: 2 (refilled) → Request arrives → Bucket: 1 ✓ Data structure:\nKey: \"{client_id}\" Value: { tokens: float, last_refill_timestamp: epoch } No background thread needed — compute tokens lazily on each request:\nelapsed = now - last_refill_timestamp tokens = min(capacity, tokens + elapsed × refill_rate) Pros Cons Allows controlled bursts (up to bucket size) Two parameters to tune (capacity + rate) Smooth rate enforcement Slightly harder to reason about than counters Very low memory (2 values per client) Industry standard (AWS, Stripe use this) Algorithm Comparison Summary Algorithm Memory Accuracy Burst Handling Complexity Fixed Window Low Weak at edges Poor Very Low Sliding Window Log High Exact Good Medium Sliding Window Counter Low Approximate Good Low Token Bucket Low Good Controlled bursts Medium v1 Decision: Token Bucket Why: It’s the industry standard. Low memory. Allows controlled bursts (which is realistic — users often send a few requests quickly, then pause). Two values per client. No background threads.\n5. Detailed Design (v1) 5.1 Core Data Model RateLimitEntry { tokens: float64 // current tokens available last_refill_time: int64 // unix timestamp (milliseconds) } RateLimitConfig { bucket_capacity: int // max burst size refill_rate: float64 // tokens per second } 5.2 Algorithm Pseudocode function is_allowed(client_id, config): entry = store.get(client_id) if entry is null: entry = { tokens: config.bucket_capacity, last_refill_time: now() } // Lazy refill elapsed = now() - entry.last_refill_time entry.tokens = min(config.bucket_capacity, entry.tokens + elapsed * config.refill_rate) entry.last_refill_time = now() if entry.tokens \u003e= 1: entry.tokens -= 1 store.set(client_id, entry) return ALLOWED // remaining = floor(entry.tokens) else: store.set(client_id, entry) return REJECTED // retry_after = (1 - entry.tokens) / config.refill_rate 5.3 HTTP Response Design Allowed (200/2xx):\nHTTP/1.1 200 OK X-RateLimit-Limit: 100 X-RateLimit-Remaining: 42 X-RateLimit-Reset: 1708425600 Rejected (429):\nHTTP/1.1 429 Too Many Requests X-RateLimit-Limit: 100 X-RateLimit-Remaining: 0 X-RateLimit-Reset: 1708425600 Retry-After: 3 Content-Type: application/json { \"error\": \"rate_limit_exceeded\", \"message\": \"Too many requests. Please retry after 3 seconds.\", \"retry_after_seconds\": 3 } 5.4 Client Identification Strategy Method Use Case Limitation API Key Authenticated APIs Doesn’t work for unauthenticated endpoints User ID Per-user limits after auth Same — requires authentication IP Address Unauthenticated / pre-auth endpoints Shared IPs (NAT, corporate proxies) punish all users behind them Composite Key \"{user_id}:{endpoint}\" Best for per-endpoint-per-user limiting v1 decision: Use API Key as primary identifier. Fall back to IP for unauthenticated endpoints.\n5.5 Configuration Example rate_limits: default: bucket_capacity: 100 refill_rate: 10 # 10 requests/sec sustained, 100 burst tiers: free: bucket_capacity: 20 refill_rate: 2 pro: bucket_capacity: 200 refill_rate: 50 enterprise: bucket_capacity: 1000 refill_rate: 200 endpoint_overrides: \"/api/v1/login\": bucket_capacity: 5 # Strict — prevent brute force refill_rate: 0.1 # 1 attempt per 10 seconds sustained \"/api/v1/search\": bucket_capacity: 30 refill_rate: 5 6. Storage Choice (v1) Option Latency Persistence Fit for v1? In-process HashMap ~nanoseconds None (lost on restart) Yes — simplest Redis ~0.5ms Optional Overkill for single node Memcached ~0.5ms None Overkill for single node v1 decision: In-process HashMap (or ConcurrentHashMap in Java / sync.Map in Go).\nEviction: Use a TTL equal to bucket_capacity / refill_rate (time to fully refill). Clients who stop sending requests get cleaned up automatically. Alternatively, run a periodic cleanup goroutine/thread every 60 seconds.\n7. Failure Mode: Fail-Open vs Fail-Closed This is a critical design decision even for v1.\nStrategy Behavior when limiter fails Risk Fail-open Allow all requests through System can be overwhelmed Fail-closed Reject all requests Legitimate users blocked v1 decision: Fail-open with alerting. Rationale: The rate limiter protects, but it should never become the reason the service is down. Log aggressively when the limiter is unavailable so ops can react.\n8. What This Design Handles Per-client rate limiting (by API key or IP) Configurable limits per tier and endpoint Controlled bursts via token bucket Clear rejection response with retry guidance Low latency (in-memory, no network hop) Bounded memory with TTL-based eviction 9. What This Design Does NOT Handle (Yet) These are intentionally deferred to keep v1 simple. Each becomes a future iteration.\nGap Why it matters Future iteration Distributed rate limiting Multiple app servers each have their own counters — a client gets N × limit v2: Centralized store (Redis) Race conditions Concurrent requests on same node can read stale token count v2: Atomic operations / Lua scripts Global rate limiting Limit across all clients (e.g., total 10K req/s to protect DB) v3: Global token bucket Sliding window precision Token bucket allows bursts; some use cases need strict per-second caps v3: Hybrid algorithm Multi-region Users hitting different data centers get separate limits v4: Cross-DC synchronization Adaptive / dynamic limits Adjusting limits based on system health (CPU, queue depth) v4: Feedback loop Request prioritization Not all requests are equal — health checks vs writes v3: Priority queues Analytics \u0026 observability Who’s being throttled? How often? v2: Metrics + dashboards 10. Quick Estimation (Back-of-Envelope) Assumptions: 1M active users, rate limit check on every request.\nDimension Calculation Result Memory per entry client_id (64B) + tokens (8B) + timestamp (8B) ~80 bytes Total memory (1M users) 80B × 1,000,000 ~80 MB Latency per check HashMap lookup + arithmetic \u003c 1 microsecond Throughput CPU-bound; single core can do millions of lookups/sec Not a bottleneck This fits comfortably in a single server’s memory. The rate limiter will never be the bottleneck at this scale.\n11. Interview Discussion Points When presenting this design, a staff/principal engineer would highlight:\n“I chose token bucket because…” — shows you evaluated alternatives and made a reasoned tradeoff. Fail-open vs fail-closed — shows you think about failure modes proactively. “This breaks down when…” — acknowledging the single-node limitation and having a clear v2 plan shows architectural maturity. Response headers — shows you think about developer experience, not just backend plumbing. Per-endpoint overrides — shows you understand that not all traffic is equal (login vs search). Next: v2 — Distributed Rate Limiting When we’re ready, the next iteration will tackle:\nMoving from in-memory to Redis (centralized store) Handling race conditions with atomic operations (Redis Lua scripts / MULTI/EXEC) Consistency vs availability tradeoffs in the rate limiter itself Observability: metrics, dashboards, alerting on throttle rates ","wordCount":"1741","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Ankul Choudhry"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ankul01.github.io/leadership-learning/system-design/hld/rate-limiter/"},"publisher":{"@type":"Organization","name":"Leadership Learning","logo":{"@type":"ImageObject","url":"https://ankul01.github.io/leadership-learning/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://ankul01.github.io/leadership-learning/ accesskey=h title="Leadership Learning (Alt + H)">Leadership Learning</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ankul01.github.io/leadership-learning/ title=Home><span>Home</span></a></li><li><a href=https://ankul01.github.io/leadership-learning/behavioral/ title=Behavioral><span>Behavioral</span></a></li><li><a href=https://ankul01.github.io/leadership-learning/system-design/ title="System Design"><span>System Design</span></a></li><li><a href=https://ankul01.github.io/leadership-learning/coding/ title=Coding><span>Coding</span></a></li><li><a href=https://ankul01.github.io/leadership-learning/frameworks/ title=Frameworks><span>Frameworks</span></a></li></ul></nav></header><div class=site-container><aside class=sidebar id=sidebar><button class=sidebar-close id=sidebar-close aria-label="Close sidebar">
<svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="width:18px;height:18px;display:block"><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></button><nav class=sidebar-nav aria-label="Section navigation"><ul class=sidebar-tree><li class=sidebar-section><details><summary><a href=/leadership-learning/architecture-deep-dive/ class=sidebar-link>Architecture Deep Dive</a></summary><ul><li class=sidebar-subsection><a href=/leadership-learning/architecture-deep-dive/migration-strategies/ class=sidebar-link>Migration Strategies</a></li><li class=sidebar-subsection><a href=/leadership-learning/architecture-deep-dive/monolith-to-microservices/ class=sidebar-link>Monolith to Microservices</a></li><li class=sidebar-subsection><a href=/leadership-learning/architecture-deep-dive/observability/ class=sidebar-link>Observability</a></li><li class=sidebar-subsection><a href=/leadership-learning/architecture-deep-dive/platform-design/ class=sidebar-link>Platform Design</a></li><li class=sidebar-subsection><a href=/leadership-learning/architecture-deep-dive/reliability/ class=sidebar-link>Reliability</a></li><li class=sidebar-subsection><a href=/leadership-learning/architecture-deep-dive/security/ class=sidebar-link>Security</a></li></ul></details></li><li class=sidebar-section><details><summary><a href=/leadership-learning/behavioral/ class=sidebar-link>Behavioral Interview Preparation</a></summary><ul><li class=sidebar-subsection><a href=/leadership-learning/behavioral/conflict-resolution/ class=sidebar-link>Conflict Resolution</a></li><li class=sidebar-subsection><a href=/leadership-learning/behavioral/decision-making/ class=sidebar-link>Decision Making</a></li><li class=sidebar-subsection><a href=/leadership-learning/behavioral/failure-stories/ class=sidebar-link>Failure Stories</a></li><li class=sidebar-subsection><details><summary><a href=/leadership-learning/behavioral/leadership/ class=sidebar-link>Leadership</a></summary><ul><li><a href=/leadership-learning/behavioral/leadership/work-customer-back/ class=sidebar-link>Work Customer Back</a></li></ul></details></li><li class=sidebar-subsection><a href=/leadership-learning/behavioral/ownership/ class=sidebar-link>Ownership</a></li><li class=sidebar-subsection><a href=/leadership-learning/behavioral/scaling-systems/ class=sidebar-link>Scaling Systems</a></li><li class=sidebar-subsection><a href=/leadership-learning/behavioral/stakeholder-management/ class=sidebar-link>Stakeholder Management</a></li></ul></details></li><li class=sidebar-section><details><summary><a href=/leadership-learning/coding/ class=sidebar-link>Coding Interview Preparation</a></summary><ul><li class=sidebar-subsection><a href=/leadership-learning/coding/concurrency/ class=sidebar-link>Concurrency</a></li><li class=sidebar-subsection><a href=/leadership-learning/coding/dsa-patterns/ class=sidebar-link>DSA Patterns</a></li><li class=sidebar-subsection><a href=/leadership-learning/coding/language-specific/ class=sidebar-link>Language Specific</a></li><li class=sidebar-subsection><a href=/leadership-learning/coding/leetcode-solutions/ class=sidebar-link>LeetCode Solutions</a></li><li class=sidebar-subsection><a href=/leadership-learning/coding/problem-solving-frameworks/ class=sidebar-link>Problem Solving Frameworks</a></li></ul></details></li><li class=sidebar-section><details><summary><a href=/leadership-learning/company-specific/ class=sidebar-link>Company Specific</a></summary><ul><li class=sidebar-subsection><a href=/leadership-learning/company-specific/amazon/ class=sidebar-link>Amazon</a></li><li class=sidebar-subsection><a href=/leadership-learning/company-specific/flipkart/ class=sidebar-link>Flipkart</a></li><li class=sidebar-subsection><a href=/leadership-learning/company-specific/google/ class=sidebar-link>Google</a></li><li class=sidebar-subsection><a href=/leadership-learning/company-specific/paypal/ class=sidebar-link>PayPal</a></li><li class=sidebar-subsection><a href=/leadership-learning/company-specific/startup-interviews/ class=sidebar-link>Startup Interviews</a></li></ul></details></li><li class=sidebar-section><details><summary><a href=/leadership-learning/engineering-leadership/ class=sidebar-link>Engineering Leadership</a></summary><ul><li class=sidebar-subsection><a href=/leadership-learning/engineering-leadership/agile-delivery/ class=sidebar-link>Agile Delivery</a></li><li class=sidebar-subsection><a href=/leadership-learning/engineering-leadership/conflict-handling/ class=sidebar-link>Conflict Handling</a></li><li class=sidebar-subsection><a href=/leadership-learning/engineering-leadership/execution-frameworks/ class=sidebar-link>Execution Frameworks</a></li><li class=sidebar-subsection><a href=/leadership-learning/engineering-leadership/hiring/ class=sidebar-link>Hiring</a></li><li class=sidebar-subsection><a href=/leadership-learning/engineering-leadership/performance-management/ class=sidebar-link>Performance Management</a></li><li class=sidebar-subsection><a href=/leadership-learning/engineering-leadership/roadmapping/ class=sidebar-link>Roadmapping</a></li><li class=sidebar-subsection><a href=/leadership-learning/engineering-leadership/stakeholder-management/ class=sidebar-link>Stakeholder Management</a></li></ul></details></li><li class=sidebar-section><details><summary><a href=/leadership-learning/frameworks/ class=sidebar-link>Frameworks</a></summary><ul><li><a href=/leadership-learning/frameworks/decision-trees/ class=sidebar-link>Decision Trees</a></li><li><a href=/leadership-learning/frameworks/risk-assessment-framework/ class=sidebar-link>Risk Assessment Framework</a></li><li><a href=/leadership-learning/frameworks/root-cause-analysis/ class=sidebar-link>Root Cause Analysis</a></li><li><a href=/leadership-learning/frameworks/star-method/ class=sidebar-link>STAR Method</a></li><li><a href=/leadership-learning/frameworks/tradeoff-matrix/ class=sidebar-link>Tradeoff Matrix</a></li></ul></details></li><li class=sidebar-section><details><summary><a href=/leadership-learning/metrics-and-impact/ class=sidebar-link>Metrics and Impact</a></summary><ul><li><a href=/leadership-learning/metrics-and-impact/business-metrics/ class=sidebar-link>Business Metrics</a></li><li><a href=/leadership-learning/metrics-and-impact/engineering-metrics/ class=sidebar-link>Engineering Metrics</a></li><li><a href=/leadership-learning/metrics-and-impact/impact-quantification/ class=sidebar-link>Impact Quantification</a></li><li><a href=/leadership-learning/metrics-and-impact/roi-models/ class=sidebar-link>ROI Models</a></li></ul></details></li><li class=sidebar-section><details><summary><a href=/leadership-learning/mock-interviews/ class=sidebar-link>Mock Interviews</a></summary><ul><li><a href=/leadership-learning/mock-interviews/feedback-logs/ class=sidebar-link>Feedback Logs</a></li><li><a href=/leadership-learning/mock-interviews/improvement-areas/ class=sidebar-link>Improvement Areas</a></li><li><a href=/leadership-learning/mock-interviews/question-bank/ class=sidebar-link>Question Bank</a></li></ul></details></li><li class=sidebar-section><details><summary><a href=/leadership-learning/quick-revision/ class=sidebar-link>Quick Revision</a></summary><ul><li class=sidebar-subsection><a href=/leadership-learning/quick-revision/1-page-system-design-summaries/ class=sidebar-link>1-Page System Design Summaries</a></li><li class=sidebar-subsection><a href=/leadership-learning/quick-revision/behavioral-1-pagers/ class=sidebar-link>Behavioral 1-Pagers</a></li><li class=sidebar-subsection><a href=/leadership-learning/quick-revision/cheat-sheets/ class=sidebar-link>Cheat Sheets</a></li><li><a href=/leadership-learning/quick-revision/final-week-prep/ class=sidebar-link>Final Week Prep</a></li></ul></details></li><li class="sidebar-section is-active"><details open><summary><a href=/leadership-learning/system-design/ class=sidebar-link>System Design Preparation</a></summary><ul><li class=sidebar-subsection><a href=/leadership-learning/system-design/caching/ class=sidebar-link>Caching</a></li><li class=sidebar-subsection><a href=/leadership-learning/system-design/case-studies/ class=sidebar-link>Case Studies</a></li><li class=sidebar-subsection><a href=/leadership-learning/system-design/databases/ class=sidebar-link>Databases</a></li><li class=sidebar-subsection><a href=/leadership-learning/system-design/distributed-systems/ class=sidebar-link>Distributed Systems</a></li><li class="sidebar-subsection is-active"><details open><summary><a href=/leadership-learning/system-design/hld/ class=sidebar-link>HLD</a></summary><ul><li><a href=/leadership-learning/system-design/hld/rate-limiter/ class="sidebar-link current">Rate Limiter — System Design</a></li></ul></details></li><li class=sidebar-subsection><a href=/leadership-learning/system-design/lld/ class=sidebar-link>LLD</a></li><li class=sidebar-subsection><a href=/leadership-learning/system-design/messaging-queues/ class=sidebar-link>Messaging Queues</a></li><li class=sidebar-subsection><a href=/leadership-learning/system-design/scalability-patterns/ class=sidebar-link>Scalability Patterns</a></li><li class=sidebar-subsection><a href=/leadership-learning/system-design/tradeoffs/ class=sidebar-link>Tradeoffs</a></li></ul></details></li></ul></nav></aside><button class=sidebar-toggle id=sidebar-toggle aria-label="Open navigation">
<svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="width:20px;height:20px;display:block"><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ankul01.github.io/leadership-learning/>Home</a>&nbsp;»&nbsp;<a href=https://ankul01.github.io/leadership-learning/system-design/>System Design Preparation</a>&nbsp;»&nbsp;<a href=https://ankul01.github.io/leadership-learning/system-design/hld/>HLD</a></div><h1 class="post-title entry-hint-parent">Rate Limiter — System Design</h1><div class=post-description>High-level design for a rate limiter: algorithms, data model, storage, and failure modes.</div><div class=post-meta><span>9 min</span>&nbsp;·&nbsp;<span>Ankul Choudhry</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-problem-statement>1. Problem Statement</a><ul><li><a href=#when-do-you-need-a-rate-limiter>When do you need a rate limiter?</a></li></ul></li><li><a href=#2-requirements>2. Requirements</a><ul><li><a href=#21-functional-requirements>2.1 Functional Requirements</a></li><li><a href=#22-non-functional-requirements-basic>2.2 Non-Functional Requirements (Basic)</a></li></ul></li><li><a href=#3-high-level-architecture-v1--single-node>3. High-Level Architecture (v1 — Single Node)</a><ul><li><a href=#where-does-the-rate-limiter-live>Where does the rate limiter live?</a></li></ul></li><li><a href=#4-rate-limiting-algorithms>4. Rate Limiting Algorithms</a><ul><li><a href=#41-fixed-window-counter>4.1 Fixed Window Counter</a></li><li><a href=#42-sliding-window-log>4.2 Sliding Window Log</a></li><li><a href=#43-sliding-window-counter-hybrid>4.3 Sliding Window Counter (Hybrid)</a></li><li><a href=#44-token-bucket>4.4 Token Bucket</a></li><li><a href=#algorithm-comparison-summary>Algorithm Comparison Summary</a></li><li><a href=#v1-decision-token-bucket>v1 Decision: Token Bucket</a></li></ul></li><li><a href=#5-detailed-design-v1>5. Detailed Design (v1)</a><ul><li><a href=#51-core-data-model>5.1 Core Data Model</a></li><li><a href=#52-algorithm-pseudocode>5.2 Algorithm Pseudocode</a></li><li><a href=#53-http-response-design>5.3 HTTP Response Design</a></li><li><a href=#54-client-identification-strategy>5.4 Client Identification Strategy</a></li><li><a href=#55-configuration-example>5.5 Configuration Example</a></li></ul></li><li><a href=#6-storage-choice-v1>6. Storage Choice (v1)</a></li><li><a href=#7-failure-mode-fail-open-vs-fail-closed>7. Failure Mode: Fail-Open vs Fail-Closed</a></li><li><a href=#8-what-this-design-handles>8. What This Design Handles</a></li><li><a href=#9-what-this-design-does-not-handle-yet>9. What This Design Does NOT Handle (Yet)</a></li><li><a href=#10-quick-estimation-back-of-envelope>10. Quick Estimation (Back-of-Envelope)</a></li><li><a href=#11-interview-discussion-points>11. Interview Discussion Points</a></li><li><a href=#next-v2--distributed-rate-limiting>Next: v2 — Distributed Rate Limiting</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p><strong>Iteration:</strong> v1 — Basic Design
<strong>Next:</strong> Distributed rate limiting, multi-tier strategies, adaptive throttling</p></blockquote><hr><h2 id=1-problem-statement>1. Problem Statement<a hidden class=anchor aria-hidden=true href=#1-problem-statement>#</a></h2><p>A rate limiter controls the number of requests a client can send to a server within a defined time window. Without one, a single client (malicious or buggy) can overwhelm the system, degrading service for everyone.</p><h3 id=when-do-you-need-a-rate-limiter>When do you need a rate limiter?<a hidden class=anchor aria-hidden=true href=#when-do-you-need-a-rate-limiter>#</a></h3><table><thead><tr><th>Scenario</th><th>Example</th></tr></thead><tbody><tr><td>Prevent abuse / DDoS</td><td>Bot hammering login endpoint</td></tr><tr><td>Protect shared resources</td><td>Database connection pool exhaustion</td></tr><tr><td>Enforce billing tiers</td><td>Free tier: 100 req/min, Pro: 10K req/min</td></tr><tr><td>Ensure fairness</td><td>One tenant on a multi-tenant platform shouldn&rsquo;t starve others</td></tr><tr><td>Cost control</td><td>Upstream API charges per call (e.g., OpenAI, Twilio)</td></tr></tbody></table><hr><h2 id=2-requirements>2. Requirements<a hidden class=anchor aria-hidden=true href=#2-requirements>#</a></h2><h3 id=21-functional-requirements>2.1 Functional Requirements<a hidden class=anchor aria-hidden=true href=#21-functional-requirements>#</a></h3><ul><li><strong>FR1:</strong> Limit requests per client (identified by user ID, API key, or IP) to N requests per time window.</li><li><strong>FR2:</strong> Return a clear rejection response (HTTP 429) when the limit is exceeded.</li><li><strong>FR3:</strong> Support configurable limits — different endpoints or user tiers can have different limits.</li><li><strong>FR4:</strong> Include rate limit metadata in response headers (<code>X-RateLimit-Limit</code>, <code>X-RateLimit-Remaining</code>, <code>X-RateLimit-Reset</code>).</li></ul><h3 id=22-non-functional-requirements-basic>2.2 Non-Functional Requirements (Basic)<a hidden class=anchor aria-hidden=true href=#22-non-functional-requirements-basic>#</a></h3><table><thead><tr><th>NFR</th><th>Target</th><th>Why it matters</th></tr></thead><tbody><tr><td><strong>Latency</strong></td><td>&lt; 1ms per check</td><td>Rate limiter sits in the hot path of every request</td></tr><tr><td><strong>Accuracy</strong></td><td>Best-effort (slight over-count is acceptable)</td><td>Exact counting is expensive; a few extra requests slipping through is fine</td></tr><tr><td><strong>Availability</strong></td><td>Should not become a single point of failure</td><td>If the limiter goes down, we need a fallback policy (fail-open vs fail-closed)</td></tr><tr><td><strong>Memory</strong></td><td>Bounded, proportional to active clients</td><td>Can&rsquo;t allocate unbounded memory per IP/user</td></tr></tbody></table><blockquote><p><strong>Parking for v2:</strong> Strong consistency across nodes, geo-distributed limiting, sub-second window granularity.</p></blockquote><hr><h2 id=3-high-level-architecture-v1--single-node>3. High-Level Architecture (v1 — Single Node)<a hidden class=anchor aria-hidden=true href=#3-high-level-architecture-v1--single-node>#</a></h2><pre tabindex=0><code>                    ┌─────────────────────────┐
                    │        Client            │
                    └────────────┬────────────┘
                                 │
                                 ▼
                    ┌─────────────────────────┐
                    │      API Gateway /       │
                    │     Reverse Proxy        │
                    │   (e.g., Nginx, Envoy)   │
                    └────────────┬────────────┘
                                 │
                                 ▼
                    ┌─────────────────────────┐
                    │    Rate Limiter          │
                    │    Middleware            │
                    │                         │
                    │  ┌───────────────────┐  │
                    │  │  In-Memory Store   │  │
                    │  │  (HashMap/Cache)   │  │
                    │  └───────────────────┘  │
                    └────────────┬────────────┘
                                 │
                          ┌──────┴──────┐
                          │             │
                       ALLOWED       REJECTED
                          │             │
                          ▼             ▼
                    ┌───────────┐  ┌──────────┐
                    │  App      │  │ HTTP 429  │
                    │  Server   │  │ Too Many  │
                    │           │  │ Requests  │
                    └───────────┘  └──────────┘
</code></pre><h3 id=where-does-the-rate-limiter-live>Where does the rate limiter live?<a hidden class=anchor aria-hidden=true href=#where-does-the-rate-limiter-live>#</a></h3><table><thead><tr><th>Option</th><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td><strong>Client-side</strong></td><td>Reduces unnecessary network calls</td><td>Can be bypassed; not trustworthy</td></tr><tr><td><strong>Server middleware</strong></td><td>Simple to implement; colocated with app logic</td><td>Coupled to app; doesn&rsquo;t protect upstream</td></tr><tr><td><strong>API Gateway</strong></td><td>Centralized; language-agnostic; protects all services</td><td>Another infra component to manage</td></tr></tbody></table><p><strong>v1 decision:</strong> Server middleware (simplest to start). We&rsquo;ll move to API Gateway in v2.</p><hr><h2 id=4-rate-limiting-algorithms>4. Rate Limiting Algorithms<a hidden class=anchor aria-hidden=true href=#4-rate-limiting-algorithms>#</a></h2><p>This is the core of the design. There are 4 well-known algorithms. Let&rsquo;s understand each and pick one for v1.</p><h3 id=41-fixed-window-counter>4.1 Fixed Window Counter<a hidden class=anchor aria-hidden=true href=#41-fixed-window-counter>#</a></h3><p><strong>How it works:</strong> Divide time into fixed windows (e.g., each minute). Maintain a counter per client per window. Increment on each request. Reject if counter > limit.</p><pre tabindex=0><code>Window: [12:00:00 — 12:01:00]   Counter: 0 → 1 → 2 → ... → 100 → REJECT

Window: [12:01:00 — 12:02:00]   Counter resets to 0
</code></pre><p><strong>Data structure:</strong></p><pre tabindex=0><code>Key: &#34;{client_id}:{window_start_timestamp}&#34;
Value: count (integer)
</code></pre><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Dead simple</td><td><strong>Boundary burst problem</strong>: 100 requests at 12:00:59 + 100 at 12:01:00 = 200 in 2 seconds</td></tr><tr><td>Low memory (1 entry per client per window)</td><td>Unfair at window edges</td></tr></tbody></table><h3 id=42-sliding-window-log>4.2 Sliding Window Log<a hidden class=anchor aria-hidden=true href=#42-sliding-window-log>#</a></h3><p><strong>How it works:</strong> Store the timestamp of every request. On a new request, remove timestamps older than <code>now - window_size</code>. If remaining count >= limit, reject.</p><pre tabindex=0><code>Timestamps: [12:00:01, 12:00:05, 12:00:12, ..., 12:00:58]
New request at 12:01:02 → remove everything before 12:00:02 → count remaining
</code></pre><p><strong>Data structure:</strong></p><pre tabindex=0><code>Key: &#34;{client_id}&#34;
Value: sorted set of timestamps
</code></pre><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Perfectly accurate sliding window</td><td><strong>High memory</strong> — stores every timestamp</td></tr><tr><td>No boundary burst problem</td><td>Cleanup overhead on every request</td></tr></tbody></table><h3 id=43-sliding-window-counter-hybrid>4.3 Sliding Window Counter (Hybrid)<a hidden class=anchor aria-hidden=true href=#43-sliding-window-counter-hybrid>#</a></h3><p><strong>How it works:</strong> Combine fixed window counter with a weighted overlap from the previous window.</p><pre tabindex=0><code>Previous window count: 80 (window was 60s, 45s have passed into current)
Current window count: 20 (15s into current window)

Estimated count = 80 × (45/60) + 20 = 60 + 20 = 80
</code></pre><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Low memory (two counters per client)</td><td>Approximate (but very close in practice)</td></tr><tr><td>Smooths out boundary bursts</td><td>Slightly more complex logic</td></tr></tbody></table><h3 id=44-token-bucket>4.4 Token Bucket<a hidden class=anchor aria-hidden=true href=#44-token-bucket>#</a></h3><p><strong>How it works:</strong> Each client has a bucket with capacity <code>B</code> tokens. Tokens are added at rate <code>R</code> per second. Each request consumes 1 token. If the bucket is empty, reject.</p><pre tabindex=0><code>Bucket capacity: 10
Refill rate: 2 tokens/sec

[t=0]  Bucket: 10  →  Request arrives  →  Bucket: 9   ✓
[t=0]  Bucket: 9   →  Request arrives  →  Bucket: 8   ✓
...
[t=0]  Bucket: 0   →  Request arrives  →  REJECTED    ✗
[t=1]  Bucket: 2 (refilled)  →  Request arrives  →  Bucket: 1  ✓
</code></pre><p><strong>Data structure:</strong></p><pre tabindex=0><code>Key: &#34;{client_id}&#34;
Value: { tokens: float, last_refill_timestamp: epoch }
</code></pre><p>No background thread needed — compute tokens lazily on each request:</p><pre tabindex=0><code>elapsed = now - last_refill_timestamp
tokens = min(capacity, tokens + elapsed × refill_rate)
</code></pre><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Allows controlled bursts (up to bucket size)</td><td>Two parameters to tune (capacity + rate)</td></tr><tr><td>Smooth rate enforcement</td><td>Slightly harder to reason about than counters</td></tr><tr><td>Very low memory (2 values per client)</td><td></td></tr><tr><td>Industry standard (AWS, Stripe use this)</td><td></td></tr></tbody></table><h3 id=algorithm-comparison-summary>Algorithm Comparison Summary<a hidden class=anchor aria-hidden=true href=#algorithm-comparison-summary>#</a></h3><table><thead><tr><th>Algorithm</th><th>Memory</th><th>Accuracy</th><th>Burst Handling</th><th>Complexity</th></tr></thead><tbody><tr><td>Fixed Window</td><td>Low</td><td>Weak at edges</td><td>Poor</td><td>Very Low</td></tr><tr><td>Sliding Window Log</td><td>High</td><td>Exact</td><td>Good</td><td>Medium</td></tr><tr><td>Sliding Window Counter</td><td>Low</td><td>Approximate</td><td>Good</td><td>Low</td></tr><tr><td>Token Bucket</td><td>Low</td><td>Good</td><td>Controlled bursts</td><td>Medium</td></tr></tbody></table><h3 id=v1-decision-token-bucket>v1 Decision: Token Bucket<a hidden class=anchor aria-hidden=true href=#v1-decision-token-bucket>#</a></h3><p><strong>Why:</strong> It&rsquo;s the industry standard. Low memory. Allows controlled bursts (which is realistic — users often send a few requests quickly, then pause). Two values per client. No background threads.</p><hr><h2 id=5-detailed-design-v1>5. Detailed Design (v1)<a hidden class=anchor aria-hidden=true href=#5-detailed-design-v1>#</a></h2><h3 id=51-core-data-model>5.1 Core Data Model<a hidden class=anchor aria-hidden=true href=#51-core-data-model>#</a></h3><pre tabindex=0><code>RateLimitEntry {
    tokens:              float64    // current tokens available
    last_refill_time:    int64      // unix timestamp (milliseconds)
}

RateLimitConfig {
    bucket_capacity:     int        // max burst size
    refill_rate:         float64    // tokens per second
}
</code></pre><h3 id=52-algorithm-pseudocode>5.2 Algorithm Pseudocode<a hidden class=anchor aria-hidden=true href=#52-algorithm-pseudocode>#</a></h3><pre tabindex=0><code>function is_allowed(client_id, config):
    entry = store.get(client_id)

    if entry is null:
        entry = { tokens: config.bucket_capacity, last_refill_time: now() }

    // Lazy refill
    elapsed = now() - entry.last_refill_time
    entry.tokens = min(config.bucket_capacity, entry.tokens + elapsed * config.refill_rate)
    entry.last_refill_time = now()

    if entry.tokens &gt;= 1:
        entry.tokens -= 1
        store.set(client_id, entry)
        return ALLOWED    // remaining = floor(entry.tokens)
    else:
        store.set(client_id, entry)
        return REJECTED   // retry_after = (1 - entry.tokens) / config.refill_rate
</code></pre><h3 id=53-http-response-design>5.3 HTTP Response Design<a hidden class=anchor aria-hidden=true href=#53-http-response-design>#</a></h3><p><strong>Allowed (200/2xx):</strong></p><pre tabindex=0><code>HTTP/1.1 200 OK
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 42
X-RateLimit-Reset: 1708425600
</code></pre><p><strong>Rejected (429):</strong></p><pre tabindex=0><code>HTTP/1.1 429 Too Many Requests
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1708425600
Retry-After: 3
Content-Type: application/json

{
    &#34;error&#34;: &#34;rate_limit_exceeded&#34;,
    &#34;message&#34;: &#34;Too many requests. Please retry after 3 seconds.&#34;,
    &#34;retry_after_seconds&#34;: 3
}
</code></pre><h3 id=54-client-identification-strategy>5.4 Client Identification Strategy<a hidden class=anchor aria-hidden=true href=#54-client-identification-strategy>#</a></h3><table><thead><tr><th>Method</th><th>Use Case</th><th>Limitation</th></tr></thead><tbody><tr><td>API Key</td><td>Authenticated APIs</td><td>Doesn&rsquo;t work for unauthenticated endpoints</td></tr><tr><td>User ID</td><td>Per-user limits after auth</td><td>Same — requires authentication</td></tr><tr><td>IP Address</td><td>Unauthenticated / pre-auth endpoints</td><td>Shared IPs (NAT, corporate proxies) punish all users behind them</td></tr><tr><td>Composite Key</td><td><code>"{user_id}:{endpoint}"</code></td><td>Best for per-endpoint-per-user limiting</td></tr></tbody></table><p><strong>v1 decision:</strong> Use API Key as primary identifier. Fall back to IP for unauthenticated endpoints.</p><h3 id=55-configuration-example>5.5 Configuration Example<a hidden class=anchor aria-hidden=true href=#55-configuration-example>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>rate_limits</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>default</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>bucket_capacity</span><span class=p>:</span><span class=w> </span><span class=m>100</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>refill_rate</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>        </span><span class=c># 10 requests/sec sustained, 100 burst</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>tiers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>free</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>bucket_capacity</span><span class=p>:</span><span class=w> </span><span class=m>20</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>refill_rate</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>pro</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>bucket_capacity</span><span class=p>:</span><span class=w> </span><span class=m>200</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>refill_rate</span><span class=p>:</span><span class=w> </span><span class=m>50</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>enterprise</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>bucket_capacity</span><span class=p>:</span><span class=w> </span><span class=m>1000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>refill_rate</span><span class=p>:</span><span class=w> </span><span class=m>200</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>endpoint_overrides</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s2>&#34;/api/v1/login&#34;</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>bucket_capacity</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>    </span><span class=c># Strict — prevent brute force</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>refill_rate</span><span class=p>:</span><span class=w> </span><span class=m>0.1</span><span class=w>       </span><span class=c># 1 attempt per 10 seconds sustained</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=s2>&#34;/api/v1/search&#34;</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>bucket_capacity</span><span class=p>:</span><span class=w> </span><span class=m>30</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>refill_rate</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>
</span></span></span></code></pre></div><hr><h2 id=6-storage-choice-v1>6. Storage Choice (v1)<a hidden class=anchor aria-hidden=true href=#6-storage-choice-v1>#</a></h2><table><thead><tr><th>Option</th><th>Latency</th><th>Persistence</th><th>Fit for v1?</th></tr></thead><tbody><tr><td>In-process HashMap</td><td>~nanoseconds</td><td>None (lost on restart)</td><td>Yes — simplest</td></tr><tr><td>Redis</td><td>~0.5ms</td><td>Optional</td><td>Overkill for single node</td></tr><tr><td>Memcached</td><td>~0.5ms</td><td>None</td><td>Overkill for single node</td></tr></tbody></table><p><strong>v1 decision:</strong> In-process HashMap (or ConcurrentHashMap in Java / <code>sync.Map</code> in Go).</p><p><strong>Eviction:</strong> Use a TTL equal to <code>bucket_capacity / refill_rate</code> (time to fully refill). Clients who stop sending requests get cleaned up automatically. Alternatively, run a periodic cleanup goroutine/thread every 60 seconds.</p><hr><h2 id=7-failure-mode-fail-open-vs-fail-closed>7. Failure Mode: Fail-Open vs Fail-Closed<a hidden class=anchor aria-hidden=true href=#7-failure-mode-fail-open-vs-fail-closed>#</a></h2><p>This is a critical design decision even for v1.</p><table><thead><tr><th>Strategy</th><th>Behavior when limiter fails</th><th>Risk</th></tr></thead><tbody><tr><td><strong>Fail-open</strong></td><td>Allow all requests through</td><td>System can be overwhelmed</td></tr><tr><td><strong>Fail-closed</strong></td><td>Reject all requests</td><td>Legitimate users blocked</td></tr></tbody></table><p><strong>v1 decision:</strong> Fail-open with alerting. Rationale: The rate limiter protects, but it should never become the reason the service is down. Log aggressively when the limiter is unavailable so ops can react.</p><hr><h2 id=8-what-this-design-handles>8. What This Design Handles<a hidden class=anchor aria-hidden=true href=#8-what-this-design-handles>#</a></h2><ul><li><input checked disabled type=checkbox> Per-client rate limiting (by API key or IP)</li><li><input checked disabled type=checkbox> Configurable limits per tier and endpoint</li><li><input checked disabled type=checkbox> Controlled bursts via token bucket</li><li><input checked disabled type=checkbox> Clear rejection response with retry guidance</li><li><input checked disabled type=checkbox> Low latency (in-memory, no network hop)</li><li><input checked disabled type=checkbox> Bounded memory with TTL-based eviction</li></ul><h2 id=9-what-this-design-does-not-handle-yet>9. What This Design Does NOT Handle (Yet)<a hidden class=anchor aria-hidden=true href=#9-what-this-design-does-not-handle-yet>#</a></h2><p>These are intentionally deferred to keep v1 simple. Each becomes a future iteration.</p><table><thead><tr><th>Gap</th><th>Why it matters</th><th>Future iteration</th></tr></thead><tbody><tr><td><strong>Distributed rate limiting</strong></td><td>Multiple app servers each have their own counters — a client gets N × limit</td><td>v2: Centralized store (Redis)</td></tr><tr><td><strong>Race conditions</strong></td><td>Concurrent requests on same node can read stale token count</td><td>v2: Atomic operations / Lua scripts</td></tr><tr><td><strong>Global rate limiting</strong></td><td>Limit across all clients (e.g., total 10K req/s to protect DB)</td><td>v3: Global token bucket</td></tr><tr><td><strong>Sliding window precision</strong></td><td>Token bucket allows bursts; some use cases need strict per-second caps</td><td>v3: Hybrid algorithm</td></tr><tr><td><strong>Multi-region</strong></td><td>Users hitting different data centers get separate limits</td><td>v4: Cross-DC synchronization</td></tr><tr><td><strong>Adaptive / dynamic limits</strong></td><td>Adjusting limits based on system health (CPU, queue depth)</td><td>v4: Feedback loop</td></tr><tr><td><strong>Request prioritization</strong></td><td>Not all requests are equal — health checks vs writes</td><td>v3: Priority queues</td></tr><tr><td><strong>Analytics & observability</strong></td><td>Who&rsquo;s being throttled? How often?</td><td>v2: Metrics + dashboards</td></tr></tbody></table><hr><h2 id=10-quick-estimation-back-of-envelope>10. Quick Estimation (Back-of-Envelope)<a hidden class=anchor aria-hidden=true href=#10-quick-estimation-back-of-envelope>#</a></h2><p>Assumptions: 1M active users, rate limit check on every request.</p><table><thead><tr><th>Dimension</th><th>Calculation</th><th>Result</th></tr></thead><tbody><tr><td><strong>Memory per entry</strong></td><td>client_id (64B) + tokens (8B) + timestamp (8B)</td><td>~80 bytes</td></tr><tr><td><strong>Total memory (1M users)</strong></td><td>80B × 1,000,000</td><td><strong>~80 MB</strong></td></tr><tr><td><strong>Latency per check</strong></td><td>HashMap lookup + arithmetic</td><td><strong>&lt; 1 microsecond</strong></td></tr><tr><td><strong>Throughput</strong></td><td>CPU-bound; single core can do millions of lookups/sec</td><td><strong>Not a bottleneck</strong></td></tr></tbody></table><p>This fits comfortably in a single server&rsquo;s memory. The rate limiter will never be the bottleneck at this scale.</p><hr><h2 id=11-interview-discussion-points>11. Interview Discussion Points<a hidden class=anchor aria-hidden=true href=#11-interview-discussion-points>#</a></h2><p>When presenting this design, a staff/principal engineer would highlight:</p><ol><li><strong>&ldquo;I chose token bucket because&mldr;&rdquo;</strong> — shows you evaluated alternatives and made a reasoned tradeoff.</li><li><strong>Fail-open vs fail-closed</strong> — shows you think about failure modes proactively.</li><li><strong>&ldquo;This breaks down when&mldr;&rdquo;</strong> — acknowledging the single-node limitation and having a clear v2 plan shows architectural maturity.</li><li><strong>Response headers</strong> — shows you think about developer experience, not just backend plumbing.</li><li><strong>Per-endpoint overrides</strong> — shows you understand that not all traffic is equal (login vs search).</li></ol><hr><h2 id=next-v2--distributed-rate-limiting>Next: v2 — Distributed Rate Limiting<a hidden class=anchor aria-hidden=true href=#next-v2--distributed-rate-limiting>#</a></h2><p>When we&rsquo;re ready, the next iteration will tackle:</p><ul><li>Moving from in-memory to Redis (centralized store)</li><li>Handling race conditions with atomic operations (Redis Lua scripts / <code>MULTI</code>/<code>EXEC</code>)</li><li>Consistency vs availability tradeoffs in the rate limiter itself</li><li>Observability: metrics, dashboards, alerting on throttle rates</li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main></div><footer class=footer><div class=footer-social-icons><a href=https://www.linkedin.com/in/ankul-choudhry target=_blank rel="noopener noreferrer me" title=LinkedIn aria-label=LinkedIn><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="width:18px;height:18px;display:block"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a href=mailto:ankulnitt@gmail.com target=_blank rel="noopener noreferrer me" title=Email aria-label=Email><svg width="18" height="18" viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="width:18px;height:18px;display:block"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg>
</a><a href=https://github.com/ankul01 target=_blank rel="noopener noreferrer me" title=GitHub aria-label=GitHub><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="width:18px;height:18px;display:block"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></div><span>&copy; 2026 <a href=https://ankul01.github.io/leadership-learning/>Leadership Learning</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg width="12" height="6" viewBox="0 0 12 6" fill="currentColor" style="width:12px;height:6px;display:block"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){var e=document.getElementById("sidebar"),t=document.getElementById("sidebar-toggle"),n=document.getElementById("sidebar-close");if(!e||!t)return;t.addEventListener("click",function(){e.classList.add("is-open")}),n&&n.addEventListener("click",function(){e.classList.remove("is-open")}),document.addEventListener("click",function(n){e.classList.contains("is-open")&&!e.contains(n.target)&&!t.contains(n.target)&&e.classList.remove("is-open")})})()</script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>